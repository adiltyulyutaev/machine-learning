{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learnining: Lab and HW 10\n",
    "### Homework Tasks:\n",
    "* Plot the error\n",
    "* Model XOR with the help of sigmoid, linear\n",
    "* Add moments rule to learning equation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "k = 1\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-k*x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1.0-sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1.0 - x**2\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def linear_prime(x):\n",
    "    return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.64237976  0.08571054 -0.31590785]\n",
      " [-0.22065101  0.23273557 -0.64574424]\n",
      " [ 0.63461946  0.85996073  0.39531015]]\n",
      "[[-0.99903261]\n",
      " [-0.90952223]\n",
      " [ 0.53149529]]\n",
      "[0 0] [0.31462653]\n",
      "[0 1] [-0.55195944]\n",
      "[1 0] [0.02094205]\n",
      "[1 1] [-0.66420753]\n",
      "epochs: 0\n",
      "epochs: 10000\n",
      "epochs: 20000\n",
      "epochs: 30000\n",
      "epochs: 40000\n",
      "epochs: 50000\n",
      "epochs: 60000\n",
      "epochs: 70000\n",
      "epochs: 80000\n",
      "epochs: 90000\n",
      "[0 0] [2.24137344e-05]\n",
      "[0 1] [0.99535591]\n",
      "[1 0] [0.99519396]\n",
      "[1 1] [-6.30903827e-07]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.activation = tanh\n",
    "        self.activation_prime = tanh_prime\n",
    "#         self.activation = linear\n",
    "#         self.activation_prime = linear_prime\n",
    "#         self.activation = sigmoid\n",
    "#         self.activation_prime = sigmoid_prime\n",
    "        \n",
    "        # Set weights\n",
    "        self.weights = []\n",
    "        # layers = [2,2,1]\n",
    "        # range of weight values (-1,1)\n",
    "        # input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
    "        \n",
    "        for i in range(1, len(layers) - 1):\n",
    "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
    "            self.weights.append(r)\n",
    "            print(r)\n",
    "        # output layer - random((2+1, 1)) : 3 x 1\n",
    "        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
    "        print(r)\n",
    "        self.weights.append(r)\n",
    "\n",
    "    def fit(self, X, y, learning_rate=0.2, epochs=100000):\n",
    "        # Add column of ones to X\n",
    "        # This is to add the bias unit to the input layer\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        X = np.concatenate((ones.T, X), axis=1)\n",
    "         \n",
    "        for k in range(epochs):\n",
    "            i = np.random.randint(X.shape[0])\n",
    "            a = [X[i]]\n",
    "\n",
    "            for l in range(len(self.weights)):\n",
    "                    dot_value = np.dot(a[l], self.weights[l])\n",
    "                    activation = self.activation(dot_value)\n",
    "                    a.append(activation)\n",
    "            # output layer\n",
    "            error = y[i] - a[-1]\n",
    "            deltas = [error * self.activation_prime(a[-1])]\n",
    "\n",
    "            # we need to begin at the second to last layer \n",
    "            # (a layer before the output layer)\n",
    "            for l in range(len(a) - 2, 0, -1): \n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))\n",
    "\n",
    "            # reverse\n",
    "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
    "            deltas.reverse()\n",
    "\n",
    "            # backpropagation\n",
    "            # 1. Multiply its output delta and input activation \n",
    "            #    to get the gradient of the weight.\n",
    "            # 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
    "            for i in range(len(self.weights)):\n",
    "                layer = np.atleast_2d(a[i])\n",
    "                delta = np.atleast_2d(deltas[i])\n",
    "                self.weights[i] += learning_rate * layer.T.dot(delta)\n",
    "\n",
    "            if k % 10000 == 0: \n",
    "                print('epochs:', k)\n",
    "\n",
    "    def predict(self, x): \n",
    "    \n",
    "        a = np.concatenate((np.ones(1).T, np.array(x)))      \n",
    "\n",
    "        for l in range(0, len(self.weights)):\n",
    "            a = self.activation(np.dot(a, self.weights[l]))\n",
    "        return a\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    nn = NeuralNetwork([2,2,1])\n",
    "    X = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "    y = np.array([0, 1, 1, 0])\n",
    "#     X = np.array([[-1, -1],\n",
    "#                   [-1, 1],\n",
    "#                   [1, -1],\n",
    "#                   [1, 1]])\n",
    "#     y = np.array([0, 1, 1, 0])\n",
    "    for e in X:\n",
    "        print(e,nn.predict(e))\n",
    "    nn.fit(X, y)\n",
    "    for e in X:\n",
    "        print(e,nn.predict(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.92307432 -0.31430643 -0.62886568]\n",
      " [ 0.61121111 -0.96330066 -0.38001911]\n",
      " [-0.34867695  0.61236022  0.43941777]]\n",
      "[[-0.23034087]\n",
      " [-0.48988274]\n",
      " [-0.9091904 ]]\n",
      "[0 0] [0.71876699]\n",
      "[0 1] [0.57912174]\n",
      "[1 0] [0.83410177]\n",
      "[1 1] [0.73023128]\n",
      "epochs: 0\n",
      "epochs: 10000\n",
      "epochs: 20000\n",
      "epochs: 30000\n",
      "epochs: 40000\n",
      "epochs: 50000\n",
      "epochs: 60000\n",
      "epochs: 70000\n",
      "epochs: 80000\n",
      "epochs: 90000\n",
      "epochs: 100000\n",
      "epochs: 110000\n",
      "epochs: 120000\n",
      "epochs: 130000\n",
      "epochs: 140000\n",
      "epochs: 150000\n",
      "epochs: 160000\n",
      "epochs: 170000\n",
      "epochs: 180000\n",
      "epochs: 190000\n",
      "epochs: 200000\n",
      "epochs: 210000\n",
      "epochs: 220000\n",
      "epochs: 230000\n",
      "epochs: 240000\n",
      "epochs: 250000\n",
      "epochs: 260000\n",
      "epochs: 270000\n",
      "epochs: 280000\n",
      "epochs: 290000\n",
      "epochs: 300000\n",
      "epochs: 310000\n",
      "epochs: 320000\n",
      "epochs: 330000\n",
      "epochs: 340000\n",
      "epochs: 350000\n",
      "epochs: 360000\n",
      "epochs: 370000\n",
      "epochs: 380000\n",
      "epochs: 390000\n",
      "epochs: 400000\n",
      "epochs: 410000\n",
      "epochs: 420000\n",
      "epochs: 430000\n",
      "epochs: 440000\n",
      "epochs: 450000\n",
      "epochs: 460000\n",
      "epochs: 470000\n",
      "epochs: 480000\n",
      "epochs: 490000\n",
      "epochs: 500000\n",
      "epochs: 510000\n",
      "epochs: 520000\n",
      "epochs: 530000\n",
      "epochs: 540000\n",
      "epochs: 550000\n",
      "epochs: 560000\n",
      "epochs: 570000\n",
      "epochs: 580000\n",
      "epochs: 590000\n",
      "epochs: 600000\n",
      "epochs: 610000\n",
      "epochs: 620000\n",
      "epochs: 630000\n",
      "epochs: 640000\n",
      "epochs: 650000\n",
      "epochs: 660000\n",
      "epochs: 670000\n",
      "epochs: 680000\n",
      "epochs: 690000\n",
      "epochs: 700000\n",
      "epochs: 710000\n",
      "epochs: 720000\n",
      "epochs: 730000\n",
      "epochs: 740000\n",
      "epochs: 750000\n",
      "epochs: 760000\n",
      "epochs: 770000\n",
      "epochs: 780000\n",
      "epochs: 790000\n",
      "epochs: 800000\n",
      "epochs: 810000\n",
      "epochs: 820000\n",
      "epochs: 830000\n",
      "epochs: 840000\n",
      "epochs: 850000\n",
      "epochs: 860000\n",
      "epochs: 870000\n",
      "epochs: 880000\n",
      "epochs: 890000\n",
      "epochs: 900000\n",
      "epochs: 910000\n",
      "epochs: 920000\n",
      "epochs: 930000\n",
      "epochs: 940000\n",
      "epochs: 950000\n",
      "epochs: 960000\n",
      "epochs: 970000\n",
      "epochs: 980000\n",
      "epochs: 990000\n",
      "[0 0] [0.49999777]\n",
      "[0 1] [0.4999986]\n",
      "[1 0] [0.50000339]\n",
      "[1 1] [0.50000422]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class XOR:\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.activation_linear = linear\n",
    "        self.activation_linear_prime = linear_prime\n",
    "        self.activation_sigmoid = sigmoid\n",
    "        self.activation_sigmoid_prime = sigmoid_prime\n",
    "        \n",
    "        # Set weights\n",
    "        self.weights = []\n",
    "        # layers = [2,2,1]\n",
    "        # range of weight values (-1,1)\n",
    "        # input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
    "        self.moments = []\n",
    "        for i in range(1, len(layers) - 1):\n",
    "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
    "            self.weights.append(r)\n",
    "            self.moments.append(np.zeros(r.shape))\n",
    "            print(r)\n",
    "        # output layer - random((2+1, 1)) : 3 x 1\n",
    "        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
    "        print(r)\n",
    "        self.weights.append(r)\n",
    "        self.moments.append(np.zeros(r.shape))\n",
    "\n",
    "    def fit(self, X, y, learning_rate=0.2, epochs=100000, gamma=0.1):\n",
    "        # Add column of ones to X\n",
    "        # This is to add the bias unit to the input layer\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        X = np.concatenate((ones.T, X), axis=1)\n",
    "        errors = []\n",
    "        \n",
    "        for k in range(epochs):\n",
    "            i = np.random.randint(X.shape[0])\n",
    "            a = [X[i]]\n",
    "            \n",
    "            for l in range(len(self.weights)-1):\n",
    "                    dot_value = np.dot(a[l], self.weights[l])\n",
    "                    activation = self.activation_linear(dot_value)\n",
    "                    a.append(activation)\n",
    "            a.append(self.activation_sigmoid(np.dot(a[-1], self.weights[-1])))\n",
    "            # output layer\n",
    "            error = y[i] - a[-1]\n",
    "            errors.append(error**2)\n",
    "            deltas = [error * self.activation_sigmoid_prime(a[-1])]\n",
    "\n",
    "            # we need to begin at the second to last layer \n",
    "            # (a layer before the output layer)\n",
    "            for l in range(len(a) - 2, 0, -1):\n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_linear_prime(a[l]))\n",
    "\n",
    "            # reverse\n",
    "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
    "            deltas.reverse()\n",
    "\n",
    "            # backpropagation\n",
    "            # 1. Multiply its output delta and input activation \n",
    "            #    to get the gradient of the weight.\n",
    "            # 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
    "            for i in range(len(self.weights)):\n",
    "                layer = np.atleast_2d(a[i])\n",
    "                delta = np.atleast_2d(deltas[i])\n",
    "                prev_weight = np.copy(self.weights[i])\n",
    "                self.weights[i] += learning_rate * layer.T.dot(delta) + gamma*self.moments[i]\n",
    "                self.moments[i] = self.weights[i]-prev_weight\n",
    "\n",
    "            if k % 10000 == 0: \n",
    "                print('epochs:', k)\n",
    "                \n",
    "        return errors\n",
    "\n",
    "    def predict(self, x): \n",
    "    \n",
    "        a = np.concatenate((np.ones(1).T, np.array(x)))      \n",
    "\n",
    "        for l in range(len(self.weights)-1):\n",
    "            a = self.activation_linear(np.dot(a, self.weights[l]))\n",
    "        return self.activation_sigmoid(np.dot(a, self.weights[-1]))\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    nn = XOR([2,2,1])\n",
    "    X = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "    y = np.array([0, 1, 1, 0])\n",
    "#     X = np.array([[-1, -1],\n",
    "#                   [-1, 1],\n",
    "#                   [1, -1],\n",
    "#                   [1, 1]])\n",
    "#     y = np.array([0, 1, 1, 0])\n",
    "    epochs = 1000000\n",
    "    for e in X:\n",
    "        print(e,nn.predict(e))\n",
    "    errors = nn.fit(X, y, epochs=epochs, learning_rate=0.2, gamma=0.1)\n",
    "    for e in X:\n",
    "        print(e,nn.predict(e))\n",
    "        \n",
    "    plt.plot(np.arange(epochs), errors)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
